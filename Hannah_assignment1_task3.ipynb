{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "def loadDataSet():\n",
    "    with open(\"wordsList.txt\", \"r\") as f:\n",
    "        data = f.read()\n",
    "    test_sub=\"\\n\"\n",
    "    res = [i.start() for i in re.finditer(test_sub, data)]\n",
    "\n",
    "    num2=0\n",
    "    postingList=[]\n",
    "    for t in range(len(res)):\n",
    "        num1=res[t]\n",
    "        #print(num2, num1)\n",
    "        line=data[num2:num1]\n",
    "        listline=line.split(\",\") #comma with no whitespace\n",
    "        #listline= list(map(str, listline))\n",
    "        #for j in range(0, len(listline)):\n",
    "        #    listline[j]=str(listline[j])\n",
    "        postingList.append(listline)\n",
    "        num2=0\n",
    "        num2+=num1\n",
    "        num2+=1\n",
    "    #print(postingList)\n",
    "    with open('classList.txt') as f:\n",
    "        classVec = [line.rstrip() for line in f]\n",
    "        classVec= list(map(int, classVec))\n",
    "    #print(classVec)\n",
    "    return postingList, classVec\n",
    "#listOPosts, listClasses = loadDataSet() \n",
    "#print(listOPosts, listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "listOPosts, listClasses = loadDataSet() \n",
    "X_train, X_test, y_train, y_test = train_test_split(listOPosts, listClasses, random_state=42, test_size=.0833, stratify=listClasses)\n",
    "#stratify parameter uses y as class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList, inputSet): \n",
    "  returnVec = [0] * len(vocabList) \n",
    "  for word in inputSet:\n",
    "    if word in vocabList: \n",
    "      returnVec[vocabList.index(word)] = 1\n",
    "    else:\n",
    "      print(\"word: %s is not in my Vocabulary!\" % word)\n",
    "  return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "  vocabSet = set([]) # create empty set for document in dataSet:\n",
    "  for document in dataSet:\n",
    "    vocabSet = vocabSet | set(document) # union of the two sets\n",
    "  return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MULTIPLE OCCURENCES OF WORDS IN TEXT\n",
    "def bagOfWords2Vec(vocabList, inputSet): \n",
    "  returnVec = [0] * len(vocabList) \n",
    "  for word in inputSet:\n",
    "    if word in vocabList: \n",
    "      returnVec[vocabList.index(word)] += 1\n",
    "  return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix, trainCategory):\n",
    "  numTrainDocs = len(trainMatrix)\n",
    "  numWords = len(trainMatrix[0])\n",
    "  pAbusive = sum(trainCategory) / float(numTrainDocs) \n",
    "  p0Num = np.zeros(numWords) # as numerator\n",
    "  p1Num = np.zeros(numWords) # as numerator \n",
    "  p0Denom = 0 # as denominator\n",
    "  p1Denom = 0 # as denominator\n",
    "  for i in range(numTrainDocs):\n",
    "    if trainCategory[i] == 1:\n",
    "      p1Num += trainMatrix[i] \n",
    "      p1Denom += sum(trainMatrix[i])\n",
    "    else:\n",
    "      p0Num += trainMatrix[i] \n",
    "      p0Denom += sum(trainMatrix[i])\n",
    "  p1Vect = p1Num / p1Denom \n",
    "  p0Vect = p0Num / p0Denom \n",
    "  return p0Vect, p1Vect, pAbusive\n",
    "#return (p0Vect, p1Vect, pAbusive)\n",
    "#if we have to return, 3 integer values, we can return a list or a tuple with these three integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # build a naive bayes classifier: step 2\n",
    "    # vec2Classify the output vector of setOfWords2Vec, say, [0, 1, ...]\n",
    "    # p0Vec, p1Vec, pAbusive are the output of trainBN0()\n",
    "def classifyNB0(vec2Classify, p0Vec, p1Vec, pAbusive): \n",
    "  p1 = prod(power(p1Vec, vec2Classify)) * pAbusive # element-wise power computation\n",
    "  p0 = prod(power(p0Vec, vec2Classify)) * (1.0 - pAbusive) \n",
    "  if p1 > p0:\n",
    "      return 1 \n",
    "  else:\n",
    "      return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hannahenv/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "from array import array\n",
    "from numpy import *\n",
    "\n",
    "\n",
    "# training\n",
    "#listOPosts, listClasses = loadDataSet() \n",
    "myVocabList = createVocabList(X_train) \n",
    "trainMat = []\n",
    "for postinDoc in X_train:\n",
    "  trainMat.append(setOfWords2Vec(myVocabList,postinDoc))\n",
    "  p0V, p1V, pAb = trainNB0(np.array(trainMat),np.array(y_train))\n",
    "\n",
    "  ''' classifying: case 1\n",
    "  testEntry = ['codeine', '15mg', 'for', '203', 'visa', 'only', 'codeine', 'methylmorphine', 'narcotic', 'opioid', 'pain', 'reliever', 'have', '15mg', '30mg', 'pills', '15mg', 'for', '203', '15mg', 'for', '385', '15mg', 'for', '562', 'visa', 'only']\n",
    "  thisDoc = np.array(bagOfWords2Vec(myVocabList, testEntry)) \n",
    "  print(testEntry, \"classified as: \", classifyNB0(thisDoc, p0V, p1V, pAb)) # out: 0 \n",
    "\n",
    "   classifying: case 2\n",
    "  testEntry = ['hydrocodone', 'vicodin', 'brand', 'watson', 'vicodin', '750', '195', '120', '570', 'brand', 'watson', '750', '195', '120', '570', 'brand', 'watson', '325', '199', '120', '588', 'noprescription', 'required', 'free', 'express', 'fedex', 'days', 'delivery', 'for', 'over', '200', 'order', 'major', 'credit', 'cards', 'check']\n",
    "  thisDoc = np.array(bagOfWords2Vec(myVocabList, testEntry)) \n",
    "  print(testEntry, \"classified as: \", classifyNB0(thisDoc, p0V, p1V, pAb)) # out: 1 '''\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'have', 'everything', 'gain', 'incredib1e', 'gains', 'length', 'inches', 'yourpenis', 'permanantly', 'amazing', 'increase', 'thickness', 'yourpenis', 'betterejacu1ation', 'control', 'experience', 'rock', 'harderecetions', 'explosive', 'intenseorgasns', 'increase', 'volume', 'ofejacu1ate', 'doctor', 'designed', 'and', 'endorsed', '100', 'herbal', '100', 'natural', '100', 'safe', 'the', 'proven', 'naturalpenisenhancement', 'that', 'works', '100', 'moneyback', 'guaranteeed'] classified as:  1\n",
      "['bargains', 'here', 'buy', 'phentermin', 'buy', 'genuine', 'phentermin', 'low', 'cost', 'visa', 'accepted', '130', '219', '292', '120', '366', '180', '513'] classified as:  1\n",
      "['there', 'was', 'guy', 'the', 'gas', 'station', 'who', 'told', 'that', 'knew', 'mandarin', 'and', 'python', 'could', 'get', 'job', 'with', 'the', 'fbi'] classified as:  0\n",
      "['codeine', '15mg', 'for', '203', 'visa', 'only', 'codeine', 'methylmorphine', 'narcotic', 'opioid', 'pain', 'reliever', 'have', '15mg', '30mg', 'pills', '15mg', 'for', '203', '15mg', 'for', '385', '15mg', 'for', '562', 'visa', 'only'] classified as:  1\n",
      "['what', 'going', 'there', 'talked', 'john', 'email', 'talked', 'about', 'some', 'computer', 'stuff', 'that', 'went', 'bike', 'riding', 'the', 'rain', 'was', 'not', 'that', 'cold', 'went', 'the', 'museum', 'yesterday', 'was', 'get', 'and', 'they', 'had', 'free', 'food', 'the', 'same', 'time', 'was', 'giants', 'game', 'when', 'got', 'done', 'had', 'take', 'the', 'train', 'with', 'all', 'the', 'giants', 'fans', 'they', 'are', 'drunk'] classified as:  0\n",
      "['linkedin', 'julius', 'requested', 'add', 'you', 'connection', 'linkedin', 'peter', 'looking', 'forward', 'the', 'book', 'accept', 'view', 'invitation', 'from', 'julius'] classified as:  0\n"
     ]
    }
   ],
   "source": [
    "#classifying\n",
    "def YtestingNB0(): \n",
    "  for postinDoc in X_test:\n",
    "    thisDoc = np.array(bagOfWords2Vec(myVocabList, postinDoc)) \n",
    "    print(postinDoc, \"classified as: \", classifyNB0(thisDoc, p0V, p1V, pAb))\n",
    "\n",
    "YtestingNB0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
